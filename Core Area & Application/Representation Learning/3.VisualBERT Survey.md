# BERT原理解析及在多模态中的应用

原文链接：[BERT原理解析及在多模态中的应用](https://zhuanlan.zhihu.com/p/142935893)

**目录**

**1.BERT基本原理解析**

- Transformer工作原理
- BERT的Pre-training tasks

**2.Multimodal BERT**

- 单流模型：VisualBERT，Unicoder-VL(AAAI2020)，VL-BERT(ICLR2020)
- 双流模型：ViLBERT(NIPS2019)，LXMERT(EMNLP2019)
- 多种Multimodal BERT对比
- 统一单模态和多模态：UNIMO(ACL2021)

**3.总结与展望**

------

### 1 BERT基本原理解析

BERT：Pre-training of Deep Bidirectional Transformers for Language Understanding[1]是双向Transformer的Encoder。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。在NLP领域的11个方向大幅刷新了精度。BERT的本质上是通过在海量的语料的基础上运行自监督学习方法为单词学习一个好的特征表示。在特定的NLP任务中，我们可以直接使用BERT的特征表示作为该任务的词嵌入特征。

#### **1.1 Transformer工作原理**

在解读BERT原理之前，我们需要了解Transformer[2]的框架。Transformer出自论文《Attention is all you need》中。Transformer是一个encoder-decoder的结构，由若干个编码器和解码器堆叠形成，如图1-1所示，左边是encoder，右边是decoder。我们将着重从Self-attention，Multi-Head Self-attention，Positional Encoding，Layer Norm四个方面讲解Transformer的结构。

![img](https://pic2.zhimg.com/80/v2-c2c678f1e224f001c35fe015c9c5cc4d_1440w.jpg)

**(1) Self-attention：**目的是学习句子内部的词依赖关系，捕获句子的内部结构。例如输入一个句子，里面的每个词都要和该句子中的所有词进行attention计算。Self-attention是基于query，key，value三个值计算的。具体流程如图1-2。

<img src="https://pic1.zhimg.com/80/v2-f5a876d570297e1fb0e2c4b188beb3b0_1440w.jpg" alt="img" style="zoom:33%;" />

具体计算流程如下：

![[公式]](https://www.zhihu.com/equation?tex=a%5E1++%3D+W+x%5E1+%3B+q%5E1+%3D+W%5Eq+a%5E1+%3B+k%5E1+%3D+W%5Ek+a%5E1+%3B+v%5E1+%3D+W%5Ev+a%5E1)

![[公式]](https://www.zhihu.com/equation?tex=a_%7B1%2Ci%7D+%3D+%5Cfrac%7Bq%5E1+%5Ccdot+k%5Ei%7D%7B%5Csqrt%7Bd%7D%7D%3B+%5Chat%7Ba_%7B1%2Ci%7D%7D+%3D+%5Cfrac%7Bexp%28a_%7B1%2Ci%7D%29%7D%7B%5Csum_j+exp%28a_%7B1%2Cj%7D%29%7D)

![[公式]](https://www.zhihu.com/equation?tex=b%5E1+%3D+%5Csum+%5Chat%7Ba_%7B1%2Ci%7D%7Dv%5Ei)

这里，q--query，k--key，v--value。

经过上述的运算过程，可以把self-attention layer看做如图1-3一样的结构。

<img src="https://pic2.zhimg.com/80/v2-b7afa5990fea4e69600b877ac8521459_1440w.jpg" alt="img" style="zoom:33%;" />

**(2) Multi-Head Self-attention：**就是我们可以有不同的Q,K,V表示，最后再将其结果结合起来。比如图1-2中，对于 ![[公式]](https://www.zhihu.com/equation?tex=a%5E1) 可以有多个q，多个k，多个v。

<img src="https://pic3.zhimg.com/80/v2-e42fbc768d53aea7d51efab7de73b8b2_1440w.jpg" alt="img"  />

 **（3）Positional Encoding：**由于模型中不包含递归和卷积，为了使模型能够利用序列的顺序，作者注入一些关于序列中标记的相对或绝对位置的信息。所以将“位置编码”添加到编码器和解码器堆栈底部的输入嵌入中。

**（4）Layer Normalization**[3]。缓解Internal Covariate Shift问题，可以将数据分布拉到激活函数的非饱和区，具有权重/数据伸缩不变性的特点。起到缓解梯度消失/爆炸、加速训练、正则化的效果。具体可以阅读他的原文。

#### **1.2 BERT的Pre-training tasks**

BERT是通过大量句子来训练的，这些句子不需要用标注。在[1]中，他们使用两种方法来训练。

**训练方法1:** Masked LM

把输入的句子中，挖空住一定比例的词汇，让BERT进行补全，预测被遮挡的词汇。补全相当于一个Linear Multi-class Classifier。如果两个词汇填在同一个位置意思相同，他们就会类似的embedding。

**训练方法2:** Next Sentence Prediction

给模型两个句子，让模型预测这两个句子是否应该接在一起。在两个句子之间会插入[SEP]，然后在两个句子前面插入[CLS]用来做二分类。

### **2 Multimodal BERT**

近些年提出很多Multimodal BERT来完成Vision-and-Language Tasks，引用[6]的表格，可以一览multimodal Bert的发展。

![img](https://pic2.zhimg.com/80/v2-705b8e54d32571f3c55dd9118a26edc1_1440w.jpg)

主要可以分为两类：

- 单流模型：在模型中将文本信息和视觉信息在一开始就进行融合。
- 双流模型：文本信息和视觉信息先分别经过两个独立的编码模块，然后通过互相的注意力机制来实现不同模态信息的融合。

我们将着重讲解部分方法，关注于文本和图像的结合方法。

#### **2.1 单流模型**

（1）VisualBERT[4]

![img](https://pic2.zhimg.com/80/v2-13ec03fd1902eeb66b5278875af5f111_1440w.jpg)VISUALBERT: A SIMPLE AND PERFORMANTB ASELINE FOR VISION AND LANGUAGE

这篇文章将文本和图像同时输入到模型中，将文字和图片信息通过 Transformer 的自注意力机制进行对齐融合。其中文本的输入和其他模型一样，以词为单位输入。这里的图像将输入到Faster-RCNN类似的模型中抽取图像的object，使用proposal的RoI特征作为每个object的特征。这里是一个无序的排列。

VisualBERT 遵循 BERT 一样的流程，先进行预训练然后在相应的任务上进行微调。

两个预训练任务：

- LM Masked
- 句子-图像预测 （即判断输入的句子是否为相应图片的描述）

作者在 VQA，VCR，NLVR2 和 Flickr30k 四个视觉语言任务上进行了测试。进一步的消融实验表明 VisualBERT 可以有效地学习到语言和相应图像区域的联系，同时也具有一定的句法敏感性。

（2）Unicoder-VL[5]

![img](https://pic1.zhimg.com/80/v2-f6ba2ec70f8abfdc9a9645d7441d8f90_1440w.jpg)Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training

与VisualBERT 极其相似，在结构上同样采用堆叠的 Transformer，并且同样在一开始就对图像和语言信息进行对齐和融合。

与 VisualBERT 最大的不同在于改模型在输入端对图像的处理。其文字部分的输入与上文相似。在图像的输入上，其首先通过 Faster-RCNN 提取区域图像特征，然后将该特征与区域图像在图像中的位置编码进行拼接再经过一个连接层投影到与语言输入维度相同的空间。

遵循先预训练后微调的模式。该模型在三个任务中进行预训练，

- 语言掩码
- 基于图像区域的掩码e类别预测
- 图像语言匹配任务

（3）VL-BERT[6]

![img](https://pic3.zhimg.com/80/v2-8882e1d1290fdc8b80edd6efef722792_1440w.jpg)Vl-bert: Pre-training of generic visual-linguistic representations

在输入端与上述两个模型略有不同。输入由以下几个编码的加和构成：Token / Visual Feature / Segment / Sequence position Embedding。

Segment embedding举例：

- <Question, Answer, Image>, A denotes Question, B denotes Answer, and C denotes Image
- <Caption, Image>, A denotes Caption, and C denotes Image.

Pre-training VL-BERT:

- Masked Language Modeling with Visual Clues
- Masked RoI Classification with Linguistic Clues

#### **2.2 双流模型**

基于双流的 ViLBERT，在一开始并未直接对语言信息和图片信息进行融合，而是先各自经过 Transformer 的编码器进行编码。分流设计是基于这样一个假设：语言的理解本身比图像复杂，而且图像的输入本身就是经过 Faster-RCNN 提取的较高层次的特征，因此两者所需要的编码深度应该是不一样的。

（1）ViL-BERT[7]

![img](https://pic4.zhimg.com/80/v2-fc16a476e4337bcb136f0b26577e2c0f_1440w.jpg)

![img](https://pic1.zhimg.com/80/v2-daf49afebe784ff318fc15f207309918_1440w.jpg)Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks

当两种模态各自进行编码后，其输出会经过一个共注意力机制模块（如下图右侧所示）。该模块也是基于 Transformer 的结构，只是在自注意力机制中每个模块都用自己的 Query 去和另一模块的 Value 和 Key 计算注意力，由此来融合不同模块间的信息。这会在视觉流中表现为图像条件下的语言注意力，在语言流中表现为语言条件下的图像注意力。

预训练：

- Masked multimodal modeling，在图像端任务的目标则是当区域图像被掩盖后模型对其输出的分类分布能够尽可能与用来提取区域特征的模型（这里是 Faster-RCNN）的输出分布一致因此这里作者使用 KL 散度作为目标函数；
- Multimodal alignment prediction: 语言图像匹配任务

作者分别在 VQA, VCR, GRE, IR, ZSIR 等五个任务中最模型进行测试。实验结果表明ViLBERT在各个任务上都提升了2~10个百分点的精度。此外，ViLBERT针对这些任务的修改很简单，所以该模型可以作为跨多个视觉和语言任务的视觉基础。其后作者又对预训练过程进行分析发现与训练过程中模型已经能够学习到语言与图像在语义上的对齐关系。

（2）LXMERT[8]

![img](https://pic1.zhimg.com/80/v2-5f4d31afccb5645349b1270bb15ae28c_1440w.jpg)LXMERT: Learning cross-modality encoder representations from transformers

该模型与 ViLBERT 一样采用了双流模型。语言与图像在一开始先各自经过独立的编码层进行编码，然后再经过一个模态交互编码层进行语言与图像在语义上的对齐和融合。

在交互编码层中，该模型同样的也是使用共注意力机制，即自注意力中的 query 来自一个模态，而 key 和 value 来自另一个模态。该编码层过后，图像与语言各自又经过一层自注意力层进一步提取高层特征。

该模型的输出有三个部分，一个语言端的输出，一个图像端的输出，一个多模态的输出。该模型在与训练时使用了四个任务：语言掩码任务，图像掩码任务（该任务有两部分，第一部分为预测被掩图像物体类别，第二部分为 ROI 特征回归任务该任务使用 L2 损失函数，语言图像匹配任务和图像问答任务。

![img](https://pic1.zhimg.com/80/v2-aa6d49119b2cf7e3df0ab7cd9c03c790_1440w.jpg)

![img](https://pic1.zhimg.com/80/v2-b5d2b2a57d488361bf7e3fa11c83c70c_1440w.jpg)

预训练任务：

- Masked cross-modality language modeling
- Masked object prediction(ROI-feature regression & detected-label classification)
- Cross-modality matching
- Image question answering

### **3.总结**

BERT是面向输入为一个或两个句子的 NLP 任务的预训练模型。为了将 BERT 架构应用于跨模态任务中，现在已有诸多处理不同模态的方法。ViLBERT和LXMERT 先分别应用一个单模态Transformer到图像和句子上，之后再采用跨模态Transformer来结合这两种模态。其他工作如VisualBERT， B2T2，Unicoder-VL， VL-BERT， Unified VLP，UNITER等等，则都是将图像和句子串联为Transformer的单个输入。很难说哪个模型架构更好，因为模型的性能非常依赖于指定的场景。



> [1] Devlin J , Chang M W , Lee K , et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[J]. 2018.
> [2] Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems. 2017.
> [3] Ba J L, Kiros J R, Hinton G E. Layer normalization[J]. arXiv preprint arXiv:1607.06450, 2016.
> [4] Li, Liunian Harold, et al. "Visualbert: A simple and performant baseline for vision and language." arXiv preprint arXiv:1908.03557 (2019).
> [5] Li, Gen, et al. "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training." AAAI2020.
> [6] Su, Weijie, et al. "Vl-bert: Pre-training of generic visual-linguistic representations." ICLR2020
> [7] Lu, Jiasen, et al. "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks." Advances in Neural Information Processing Systems. 2019.
> [8] LXMERT: Learning cross-modality encoder representations from transformers. Hao Tan, Mohit Bansal,EMNLP2019